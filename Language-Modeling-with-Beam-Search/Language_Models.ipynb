{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language-Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU55v7dA-tEQ",
        "colab_type": "text"
      },
      "source": [
        "# LSTM based Language Model with Beam Search\n",
        "Abhimanyu Talwar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTvHp9KZ2Hfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "from torchtext.datasets import WikiText2\n",
        "\n",
        "from torchtext.data.iterator import BPTTIterator\n",
        "from torchtext.data import Batch, Dataset\n",
        "import math\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5duJHXt8UOf",
        "colab_type": "text"
      },
      "source": [
        "# Download Dataset & Create Train, Validation, Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h_5rIbn4c5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up fields\n",
        "TEXT = torchtext.data.Field(lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0zOx2dS4ddU",
        "colab_type": "code",
        "outputId": "1311a564-a1f5-43ae-f10b-cb62afb72d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# make splits for data\n",
        "train, val, test = WikiText2.splits(TEXT)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 17.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVr7DHZJ8ywW",
        "colab_type": "code",
        "outputId": "9f2ce99c-3d19-49a4-b230-33a657876889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "for set_, name_ in zip([train, val, test], ['Train', 'Validation', 'Test']):\n",
        "    print('Length of {} Set is {}'.format(name_, len(set_)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Train Set is 1\n",
            "Length of Validation Set is 1\n",
            "Length of Test Set is 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJInogMB9eEu",
        "colab_type": "code",
        "outputId": "456c8581-7a08-41cd-8774-94d300a6ebfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_SIZE = 30000\n",
        "if MAX_SIZE is None:\n",
        "    TEXT.build_vocab(train)\n",
        "else:\n",
        "    TEXT.build_vocab(train, max_size=MAX_SIZE)\n",
        "print('Vocabulary Length: ', len(TEXT.vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Length:  28913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQlVKJs-_ytk",
        "colab_type": "code",
        "outputId": "45148f42-9ab0-4f25-ffb3-1c1e42d33fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print('Pad Token:     ', TEXT.pad_token)\n",
        "print('Unknown Token: ', TEXT.unk_token)\n",
        "print('EOS Token:     ', TEXT.eos_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad Token:      <pad>\n",
            "Unknown Token:  <unk>\n",
            "EOS Token:      None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-kKDTHdH9ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = BPTTIterator.splits((train, val, test), \\\n",
        "                               batch_size=32, device=torch.device(\"cuda\"), bptt_len=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoI7xeZaAvqE",
        "colab_type": "code",
        "outputId": "3e160cdd-7370-4bcf-a7c0-f84ffeb255a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it) \n",
        "print(\"Size of text batch [max bptt length, batch size]\", batch.text.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of text batch [max bptt length, batch size] torch.Size([30, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmtGLmkhIbWU",
        "colab_type": "code",
        "outputId": "c156b328-af1d-4c1d-fe8e-4ce6afe1eb1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "print('='*90)\n",
        "print('Raw form of [batch.text]: ')\n",
        "print(' '*15, batch.text[0,:])\n",
        "print('='*90)\n",
        "print('Text form of [batch.text]: ')\n",
        "print(' '*15, ' '.join([TEXT.vocab.itos[i] for i in batch.text[:,0].cpu().numpy()]))\n",
        "print('='*90)\n",
        "print('Text form of [batch.target]: ')\n",
        "print(' '*15, ' '.join([TEXT.vocab.itos[i] for i in batch.target[:,0].cpu().numpy()]))\n",
        "print('='*90)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Raw form of [batch.text]: \n",
            "                tensor([    9,  1499,    57,    26,  1439,  7863,   159,   180,    28,   691,\n",
            "          235,     8,  1288,    59, 13430,    47,    12,    85,     6,  5979,\n",
            "        16940,  2492,    19,     3,     5,  4912,     4, 24526,    30,     7,\n",
            "           23,    36], device='cuda:0')\n",
            "==========================================================================================\n",
            "Text form of [batch.text]: \n",
            "                <eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 : <unk> chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly\n",
            "==========================================================================================\n",
            "Text form of [batch.target]: \n",
            "                = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 : <unk> chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wRl2irJM6I",
        "colab_type": "text"
      },
      "source": [
        "# Declare Model and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuKrYpJx-OjC",
        "colab_type": "text"
      },
      "source": [
        "## Functions for predicting last word and generating a random sentence  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1dY_cetutwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PredictLastWord(model, sentence_txt):\n",
        "    \"\"\"\n",
        "    INPUTS:\n",
        "        sentence_txt:  string, a sentence whose last word we want to predict\n",
        "                       e.g. 'he is eating a'\n",
        "    OUTPUTS:\n",
        "        predictions:   list, of top 20 words (strings) which could be the last\n",
        "                       word of sentence_txt\n",
        "        top_log_probs: list, of the log probabilities for the top 20 predicted\n",
        "                       words\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sent_tokens = sentence_txt.split(' ')\n",
        "    indices = [TEXT.vocab.stoi[sent_tokens[-i]] for i in range(len(sent_tokens), 0, -1)]\n",
        "    #print(' '.join([TEXT.vocab.itos[i] for i in indices]))\n",
        "    indices = np.expand_dims(np.asarray(indices), axis=1)\n",
        "    inputs = torch.from_numpy(indices).to('cuda')\n",
        "    log_probs, _ = model(inputs)\n",
        "    top_log_probs, topidx = log_probs[-1,0,:].topk(20)\n",
        "    predictions = [TEXT.vocab.itos[i] for i in topidx.cpu().numpy()]\n",
        "    return top_log_probs, predictions\n",
        "\n",
        "def GenerateRandom(model, seed_word, sentence_len=10):\n",
        "    model.eval()\n",
        "    bad_words = ['<eos>', '<unk>', '\"', '.', '@-@', '@.@']\n",
        "    bad_tokens = [TEXT.vocab.stoi[w] for w in bad_words]\n",
        "    minus_inf = -1e4\n",
        "    \n",
        "    sentence_ = [seed_word]\n",
        "    idx = TEXT.vocab.stoi[seed_word]\n",
        "    lstm_states = None\n",
        "    for i in range(sentence_len):\n",
        "        idx = np.expand_dims(np.asarray([idx]), axis=1)\n",
        "        inputs = torch.from_numpy(idx).to('cuda')\n",
        "\n",
        "        log_probs, (h, c) = model(inputs, lstm_states=lstm_states)\n",
        "        log_probs[:,:,bad_tokens] = minus_inf\n",
        "        \n",
        "        top_log_prob, topidx = log_probs[-1,0,:].topk(10)\n",
        "        topidx = np.random.choice(topidx.cpu().numpy(), size=1)[0]\n",
        "        \n",
        "        new_word = TEXT.vocab.itos[topidx]\n",
        "        sentence_ += [new_word]\n",
        "        lstm_states = (h, c)\n",
        "        idx = topidx\n",
        "    return ' '.join(sentence_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS4n3-BU-Vem",
        "colab_type": "text"
      },
      "source": [
        "## Beam Search Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSZZk4uWnE6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BeamSearch(model, seed_phrase, beam_size=100, sentence_len=10):\n",
        "    \"\"\"\n",
        "    INPUTS:\n",
        "        seed_phrase:  string, which denotes start of a sentence, \n",
        "                      e.g. 'he is a'\n",
        "        sentence_len: integer, number of words to be generated in \n",
        "                      addition to the seed_phrase\n",
        "        beam_size:    integer, size of the beam for Beam Search\n",
        "    OUTPUT:\n",
        "        result:       list, of beam_size number of sentences generated\n",
        "                      using Beam Search\n",
        "    \"\"\"\n",
        "    def IdxToTensor(indices):\n",
        "        indices = np.expand_dims(np.asarray(indices), axis=1)\n",
        "        tensor_idx = torch.from_numpy(indices).to('cuda')\n",
        "        return tensor_idx\n",
        "    \n",
        "    \n",
        "    minus_inf = -1e4\n",
        "    model.eval()\n",
        "    bad_words = ['<eos>', '<unk>', '.', '\"', '@-@', '@.@']\n",
        "    bad_tokens = [TEXT.vocab.stoi[w] for w in bad_words]\n",
        "    \n",
        "    lstm_states = None\n",
        "    seed_words = seed_phrase.split()\n",
        "    seed_indices = [TEXT.vocab.stoi[w] for w in seed_words]\n",
        "    for word in seed_words:\n",
        "        word_idx = TEXT.vocab.stoi[word]\n",
        "        inputs = IdxToTensor([word_idx])\n",
        "        log_probs, (h, c) = model(inputs, lstm_states=lstm_states)\n",
        "        lstm_states = (h, c)\n",
        "    \n",
        "    log_probs[:,:,bad_tokens] = minus_inf\n",
        "    \n",
        "    top_log_probs, top_idx = log_probs.flatten().topk(beam_size)\n",
        "    \n",
        "    beam = [top_idx.tolist()]\n",
        "    beam_data = [(seed_indices + [b], lstm_states) for b in beam[0]]\n",
        "    running_log_probs = top_log_probs.unsqueeze(dim=1).unsqueeze(dim=0)\n",
        "    \n",
        "    for i in range(sentence_len):\n",
        "        h_in, c_in = [], []\n",
        "        for j, _ in enumerate(beam[0]):\n",
        "            _, (h_, c_) = beam_data[j]\n",
        "            h_in += [h_]\n",
        "            c_in += [c_]\n",
        "\n",
        "        h_in = torch.cat(h_in, dim=1)\n",
        "        c_in = torch.cat(c_in, dim=1)\n",
        "        \n",
        "        inputs = torch.from_numpy(np.asarray(beam)).to('cuda')\n",
        "        \n",
        "        log_probs_out, (h_out, c_out) = model(inputs, (h_in, c_in))\n",
        "        log_probs_out[:,:,bad_tokens] = minus_inf\n",
        "        \n",
        "        updated_log_probs = log_probs_out + running_log_probs\n",
        "        \n",
        "        if i < sentence_len - 1:\n",
        "            running_log_probs, top_idx = updated_log_probs.flatten().topk(beam_size)\n",
        "        else:\n",
        "            running_log_probs, top_idx = updated_log_probs.flatten().topk(beam_size)\n",
        "        \n",
        "        running_log_probs = running_log_probs.unsqueeze(dim=1).unsqueeze(dim=0)\n",
        "        \n",
        "        word_indices = [w % len(TEXT.vocab) for w in top_idx.tolist()]\n",
        "        beam_indices = [w // len(TEXT.vocab) for w in top_idx.tolist()]\n",
        "\n",
        "        new_beam_data = []\n",
        "        for b_i, w_i in zip(beam_indices, word_indices):\n",
        "            new_beam_data += [(beam_data[b_i][0] + [w_i], (h_out[:,b_i:b_i+1,:], c_out[:,b_i:b_i+1,:]))]\n",
        "        \n",
        "        beam = [word_indices]\n",
        "        beam_data = new_beam_data\n",
        "    result = [' '.join([TEXT.vocab.itos[idx] for idx in x[0][0:sentence_len+len(seed_words)]]) for k, x in enumerate(beam_data)]\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piah6upx-YoV",
        "colab_type": "text"
      },
      "source": [
        "## Function for training a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "180L79rVQKCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TrainModel(model, criterion, optimizer, train_iter, val_iter, \\\n",
        "               scheduler=None, num_epochs=1, update_freq=1, grad_norm=0.1):\n",
        "    \n",
        "    def PrintStatus(prefix_, epoch_, batch_, num_batches_, train_loss, \\\n",
        "                    val_loss=None, epoch_time=None):\n",
        "        epoch_str = 'Epoch {}'.format(epoch_)\n",
        "        batch_str = 'Batch {}/{}'.format(batch_, num_batches_)\n",
        "        offset_1 = ' '*(10 - len(epoch_str))\n",
        "        offset_2 = ' '*(15 - len(batch_str)) + ':  '\n",
        "        value_str = 'Train Loss {:.3f}'.\\\n",
        "                      format(train_loss)\n",
        "        status_str = prefix_ + epoch_str + offset_1 + batch_str + offset_2 + value_str\n",
        "        if val_loss is not None:\n",
        "            status_str += ' Valid Loss {:.3f}'.format(val_loss)\n",
        "        if epoch_time is not None:\n",
        "            status_str += ' [Time {:.1f}s]'.format(epoch_time)\n",
        "        sys.stdout.write(status_str)\n",
        "    \n",
        "    \n",
        "    num_batches = len(train_iter)\n",
        "    num_val_batches = len(val_iter)\n",
        "    best_val_loss = None\n",
        "    for epoch in range(num_epochs):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        running_loss = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            inputs, labels = batch.text, batch.target\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "            outputs, _ = model(inputs)\n",
        "            \n",
        "            seq_len_ = outputs.shape[0]\n",
        "            batch_size_ = outputs.shape[1]\n",
        "            loss = criterion(outputs.reshape(batch_size_*seq_len_, -1), \\\n",
        "                             labels.reshape(batch_size_*seq_len_, -1).squeeze())\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_norm)\n",
        "            \n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "            if i % update_freq == update_freq - 1:\n",
        "                train_loss = running_loss/update_freq\n",
        "                if i < update_freq:\n",
        "                    prefix_ = '\\n'\n",
        "                else:\n",
        "                    prefix_ = '\\r'\n",
        "                PrintStatus(prefix_, epoch+1, i+1, num_batches, train_loss, \\\n",
        "                            None, time.time() - t0)\n",
        "                running_loss = 0.0\n",
        "        # Calculate validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for j, batch in enumerate(val_iter):\n",
        "            inputs, labels = batch.text, batch.target\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "            outputs, _ = model(inputs)\n",
        "            \n",
        "            seq_len_ = outputs.shape[0]\n",
        "            batch_size_ = outputs.shape[1]\n",
        "            loss = criterion(outputs.reshape(batch_size_*seq_len_, -1), \\\n",
        "                             labels.reshape(batch_size_*seq_len_, -1).squeeze())\n",
        "            val_loss += loss.item()\n",
        "        val_loss = val_loss/num_val_batches\n",
        "        if best_val_loss is None or val_loss < best_val_loss:\n",
        "            torch.save(model.state_dict(), 'best_lm_model.pth')\n",
        "            best_val_loss = val_loss\n",
        "        PrintStatus('\\r', epoch+1, num_batches, num_batches, epoch_loss/num_batches, \\\n",
        "                    val_loss, time.time() - t0)\n",
        "        # Generate random sentence from Language Model\n",
        "        sample_ = GenerateRandom(model, 'the')\n",
        "        print('\\nRandom sample: ', sample_)\n",
        "        model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DkyT4_D-cat",
        "colab_type": "text"
      },
      "source": [
        "## Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za1qIxKBL35F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, drop_rate=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.embed_layer = nn.Embedding(num_embeddings=vocab_size, \\\n",
        "                                        embedding_dim=embedding_dim)\n",
        "        # BPTTIterator generates samples in (SEQ, BATCH) order and that is\n",
        "        # why batch_first is set to False below (it is the default setting too).\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \\\n",
        "                           num_layers=num_layers, batch_first=False)\n",
        "        self.linear = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
        "        self.dropout2 = nn.Dropout(drop_rate)\n",
        "        self.logsoft = nn.LogSoftmax(dim=2)\n",
        "    \n",
        "    def forward(self, x, lstm_states=None):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.dropout(x)\n",
        "        if lstm_states is None:\n",
        "            x, (h_n, c_n) = self.lstm(x)\n",
        "        else:\n",
        "            x, (h_n, c_n) = self.lstm(x, lstm_states)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear(x)\n",
        "        log_probs = self.logsoft(x)\n",
        "        return log_probs, (h_n, c_n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3B5n_CQRjLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 1024\n",
        "NUM_LAYERS = 2\n",
        "UPDATE_FREQ = 250\n",
        "NUM_EPOCHS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pm1j0MFRTld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTMModel(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, \\\n",
        "                      hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS)\n",
        "model = model.to('cuda')\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hfb_s1Ra85",
        "colab_type": "code",
        "outputId": "d0b9ab6a-ce9e-4fe4-a5f1-c70f333046a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5217
        }
      },
      "source": [
        "TrainModel(model, criterion, optimizer, train_iter, val_iter, scheduler=scheduler, \\\n",
        "           update_freq=UPDATE_FREQ, num_epochs=NUM_EPOCHS, grad_norm=10.0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1   Batch 2176/2176:  Train Loss 6.252 Valid Loss 5.647 [Time 188.4s]\n",
            "Random sample:  the 766th division and a number for its original area on\n",
            "\n",
            "Epoch 2   Batch 2176/2176:  Train Loss 5.776 Valid Loss 5.450 [Time 193.9s]\n",
            "Random sample:  the united league , as well to the first year in\n",
            "\n",
            "Epoch 3   Batch 2176/2176:  Train Loss 5.607 Valid Loss 5.351 [Time 194.0s]\n",
            "Random sample:  the song had become an active for her best friend to\n",
            "\n",
            "Epoch 4   Batch 2176/2176:  Train Loss 5.494 Valid Loss 5.286 [Time 194.4s]\n",
            "Random sample:  the school in an english manner ; the portuguese is also\n",
            "\n",
            "Epoch 5   Batch 2176/2176:  Train Loss 5.404 Valid Loss 5.238 [Time 194.7s]\n",
            "Random sample:  the most important of a single level in their final game\n",
            "\n",
            "Epoch 6   Batch 2176/2176:  Train Loss 5.326 Valid Loss 5.200 [Time 194.7s]\n",
            "Random sample:  the united kingdom on august 15 ( 2012 and 26 september\n",
            "\n",
            "Epoch 7   Batch 2176/2176:  Train Loss 5.258 Valid Loss 5.173 [Time 194.7s]\n",
            "Random sample:  the second generation and an american revolutionary forces of wales was\n",
            "\n",
            "Epoch 8   Batch 2176/2176:  Train Loss 5.197 Valid Loss 5.145 [Time 194.4s]\n",
            "Random sample:  the city was completed in 2005 by june 2005 , with\n",
            "\n",
            "Epoch 9   Batch 2176/2176:  Train Loss 5.143 Valid Loss 5.123 [Time 194.4s]\n",
            "Random sample:  the same time and had previously received several other academy ,\n",
            "\n",
            "Epoch 10  Batch 2176/2176:  Train Loss 5.089 Valid Loss 5.111 [Time 194.8s]\n",
            "Random sample:  the game and his wife , in order to continue his\n",
            "\n",
            "Epoch 11  Batch 2176/2176:  Train Loss 5.041 Valid Loss 5.100 [Time 195.0s]\n",
            "Random sample:  the jin 's capital , a large force known in their\n",
            "\n",
            "Epoch 12  Batch 2176/2176:  Train Loss 4.996 Valid Loss 5.086 [Time 195.0s]\n",
            "Random sample:  the game is to provide an emotional level , as they\n",
            "\n",
            "Epoch 13  Batch 2176/2176:  Train Loss 4.952 Valid Loss 5.075 [Time 194.7s]\n",
            "Random sample:  the first episode for her second child on june 19 ,\n",
            "\n",
            "Epoch 14  Batch 2176/2176:  Train Loss 4.910 Valid Loss 5.069 [Time 194.6s]\n",
            "Random sample:  the 766th was to take over a major part and a\n",
            "\n",
            "Epoch 15  Batch 2176/2176:  Train Loss 4.870 Valid Loss 5.069 [Time 195.3s]\n",
            "Random sample:  the film received mostly reviews in its own style and best\n",
            "\n",
            "Epoch 16  Batch 2176/2176:  Train Loss 4.834 Valid Loss 5.062 [Time 195.3s]\n",
            "Random sample:  the united army was sent by a new mayor on 7\n",
            "\n",
            "Epoch 17  Batch 2176/2176:  Train Loss 4.797 Valid Loss 5.060 [Time 194.6s]\n",
            "Random sample:  the film and had sold one more than three hours earlier\n",
            "\n",
            "Epoch 18  Batch 2176/2176:  Train Loss 4.763 Valid Loss 5.062 [Time 194.6s]\n",
            "Random sample:  the song dynasty of new mexico and its construction of the\n",
            "\n",
            "Epoch 19  Batch 2176/2176:  Train Loss 4.728 Valid Loss 5.060 [Time 195.0s]\n",
            "Random sample:  the same thing as he has been recorded in an early\n",
            "\n",
            "Epoch 20  Batch 2176/2176:  Train Loss 4.693 Valid Loss 5.063 [Time 194.4s]\n",
            "Random sample:  the united church 's representatives and their homes were only lightly\n",
            "\n",
            "Epoch 21  Batch 2176/2176:  Train Loss 4.662 Valid Loss 5.051 [Time 195.0s]\n",
            "Random sample:  the song and the only piece that had previously served during\n",
            "\n",
            "Epoch 22  Batch 2176/2176:  Train Loss 4.631 Valid Loss 5.056 [Time 194.7s]\n",
            "Random sample:  the original story , which he had no doubt in their\n",
            "\n",
            "Epoch 23  Batch 2176/2176:  Train Loss 4.601 Valid Loss 5.059 [Time 194.8s]\n",
            "Random sample:  the same period the 766th and northern front continued on august\n",
            "\n",
            "Epoch 24  Batch 2176/2176:  Train Loss 4.572 Valid Loss 5.062 [Time 194.8s]\n",
            "Random sample:  the second wave 's 5th infantry regiments was sent by a\n",
            "\n",
            "Epoch 25  Batch 2176/2176:  Train Loss 4.543 Valid Loss 5.064 [Time 194.4s]\n",
            "Random sample:  the first time of the original season was broadcast by netflix\n",
            "\n",
            "Epoch 26  Batch 2176/2176:  Train Loss 4.515 Valid Loss 5.064 [Time 194.9s]\n",
            "Random sample:  the united church of maryland as it has become one city\n",
            "\n",
            "Epoch 27  Batch 2176/2176:  Train Loss 4.487 Valid Loss 5.070 [Time 194.9s]\n",
            "Random sample:  the song in may 2008 in its entirety ; it has\n",
            "\n",
            "Epoch 28  Batch 2176/2176:  Train Loss 4.462 Valid Loss 5.072 [Time 194.4s]\n",
            "Random sample:  the song capital was a minor source between the song in\n",
            "\n",
            "Epoch 29  Batch 2176/2176:  Train Loss 4.438 Valid Loss 5.082 [Time 194.9s]\n",
            "Random sample:  the episode was actually a week of her career at that\n",
            "\n",
            "Epoch 30  Batch 2176/2176:  Train Loss 4.378 Valid Loss 5.027 [Time 194.8s]\n",
            "Random sample:  the most famous series , it is possible on an average\n",
            "\n",
            "Epoch 31  Batch 2176/2176:  Train Loss 4.321 Valid Loss 5.023 [Time 195.1s]\n",
            "Random sample:  the north american frontier to become an industrial doctorate on june\n",
            "\n",
            "Epoch 32  Batch 2176/2176:  Train Loss 4.295 Valid Loss 5.025 [Time 194.9s]\n",
            "Random sample:  the same year and the film became increasingly positive to contemporary\n",
            "\n",
            "Epoch 33  Batch 2176/2176:  Train Loss 4.274 Valid Loss 5.025 [Time 194.4s]\n",
            "Random sample:  the city 's infrastructure are not entirely in terms for an\n",
            "\n",
            "Epoch 34  Batch 2176/2176:  Train Loss 4.258 Valid Loss 5.026 [Time 194.3s]\n",
            "Random sample:  the same day on 1 may , in the united sector\n",
            "\n",
            "Epoch 35  Batch 2176/2176:  Train Loss 4.242 Valid Loss 5.024 [Time 194.2s]\n",
            "Random sample:  the same day as its vice president general ramón smith in\n",
            "\n",
            "Epoch 36  Batch 2176/2176:  Train Loss 4.227 Valid Loss 5.027 [Time 194.9s]\n",
            "Random sample:  the song dynasty , in his words : a guide in\n",
            "\n",
            "Epoch 37  Batch 2176/2176:  Train Loss 4.214 Valid Loss 5.023 [Time 194.4s]\n",
            "Random sample:  the film is an award , not only with it for\n",
            "\n",
            "Epoch 38  Batch 2176/2176:  Train Loss 4.203 Valid Loss 5.024 [Time 194.7s]\n",
            "Random sample:  the most successful of them to do a lot , but\n",
            "\n",
            "Epoch 39  Batch 2176/2176:  Train Loss 4.189 Valid Loss 5.024 [Time 194.4s]\n",
            "Random sample:  the song court , and the mongols , which the māori\n",
            "\n",
            "Epoch 40  Batch 2176/2176:  Train Loss 4.178 Valid Loss 5.023 [Time 194.2s]\n",
            "Random sample:  the end line of each day in its path ; the\n",
            "\n",
            "Epoch 41  Batch 2176/2176:  Train Loss 4.168 Valid Loss 5.027 [Time 194.3s]\n",
            "Random sample:  the same as a young boy 's , as well during\n",
            "\n",
            "Epoch 42  Batch 2176/2176:  Train Loss 4.157 Valid Loss 5.026 [Time 194.2s]\n",
            "Random sample:  the british microlight of british army under brigadier force at anzac\n",
            "\n",
            "Epoch 43  Batch 2176/2176:  Train Loss 4.148 Valid Loss 5.024 [Time 194.7s]\n",
            "Random sample:  the north tarrytown were able to gain further progress to a\n",
            "\n",
            "Epoch 44  Batch 2176/2176:  Train Loss 4.137 Valid Loss 5.027 [Time 194.3s]\n",
            "Random sample:  the same year ) and in its original history to form\n",
            "\n",
            "Epoch 45  Batch 2176/2176:  Train Loss 4.128 Valid Loss 5.026 [Time 194.2s]\n",
            "Random sample:  the most common material to achieve the same name or as\n",
            "\n",
            "Epoch 46  Batch 2176/2176:  Train Loss 4.118 Valid Loss 5.027 [Time 194.6s]\n",
            "Random sample:  the second world , a mod which had to be included\n",
            "\n",
            "Epoch 47  Batch 2176/2176:  Train Loss 4.109 Valid Loss 5.029 [Time 194.8s]\n",
            "Random sample:  the same period as one member , and has since been\n",
            "\n",
            "Epoch 48  Batch 2176/2176:  Train Loss 4.101 Valid Loss 5.031 [Time 194.6s]\n",
            "Random sample:  the first world indoor , he began the career at school\n",
            "\n",
            "Epoch 49  Batch 2176/2176:  Train Loss 4.093 Valid Loss 5.028 [Time 194.6s]\n",
            "Random sample:  the most famous , in which dershowitz , however , was\n",
            "\n",
            "Epoch 50  Batch 2176/2176:  Train Loss 4.085 Valid Loss 5.029 [Time 194.6s]\n",
            "Random sample:  the north and northeast in south dakota ( modern georgian countries\n",
            "\n",
            "Epoch 51  Batch 2176/2176:  Train Loss 4.075 Valid Loss 5.029 [Time 194.8s]\n",
            "Random sample:  the north and the north koreans of north kingstown , but\n",
            "\n",
            "Epoch 52  Batch 2176/2176:  Train Loss 4.067 Valid Loss 5.031 [Time 194.7s]\n",
            "Random sample:  the british government was unable from that , but that it\n",
            "\n",
            "Epoch 53  Batch 2176/2176:  Train Loss 4.059 Valid Loss 5.029 [Time 194.9s]\n",
            "Random sample:  the north of heaven 's life were in greenland and was\n",
            "\n",
            "Epoch 54  Batch 2176/2176:  Train Loss 4.051 Valid Loss 5.032 [Time 194.7s]\n",
            "Random sample:  the united army from britain on 21 june , but o\n",
            "\n",
            "Epoch 55  Batch 2176/2176:  Train Loss 4.044 Valid Loss 5.031 [Time 194.5s]\n",
            "Random sample:  the first game is an ensemble that was intended as it\n",
            "\n",
            "Epoch 56  Batch 2176/2176:  Train Loss 4.035 Valid Loss 5.035 [Time 194.5s]\n",
            "Random sample:  the most important sources , a product which may have survived\n",
            "\n",
            "Epoch 57  Batch 2176/2176:  Train Loss 4.027 Valid Loss 5.032 [Time 194.6s]\n",
            "Random sample:  the song is written on her second studio and is in\n",
            "\n",
            "Epoch 58  Batch 2176/2176:  Train Loss 4.019 Valid Loss 5.035 [Time 194.6s]\n",
            "Random sample:  the same day that they could have a high pressure system\n",
            "\n",
            "Epoch 59  Batch 2176/2176:  Train Loss 4.012 Valid Loss 5.035 [Time 194.7s]\n",
            "Random sample:  the first time he would travel from another planet that ,\n",
            "\n",
            "Epoch 60  Batch 2176/2176:  Train Loss 4.086 Valid Loss 5.003 [Time 194.9s]\n",
            "Random sample:  the city and a railroad for an extension by edward middleton\n",
            "\n",
            "Epoch 61  Batch 2176/2176:  Train Loss 4.102 Valid Loss 4.990 [Time 195.3s]\n",
            "Random sample:  the episode was the best of the game , which aired\n",
            "\n",
            "Epoch 62  Batch 2176/2176:  Train Loss 4.093 Valid Loss 4.993 [Time 194.8s]\n",
            "Random sample:  the song and was performed for her song on the deluxe\n",
            "\n",
            "Epoch 63  Batch 2176/2176:  Train Loss 4.088 Valid Loss 4.995 [Time 194.5s]\n",
            "Random sample:  the same date , when the starter arrives to a full\n",
            "\n",
            "Epoch 64  Batch 2176/2176:  Train Loss 4.081 Valid Loss 4.996 [Time 194.4s]\n",
            "Random sample:  the film , was the only song that was recorded with\n",
            "\n",
            "Epoch 65  Batch 2176/2176:  Train Loss 4.080 Valid Loss 4.995 [Time 194.7s]\n",
            "Random sample:  the song to a positive song about his past the band\n",
            "\n",
            "Epoch 66  Batch 2176/2176:  Train Loss 4.079 Valid Loss 4.995 [Time 194.5s]\n",
            "Random sample:  the film has a long , with more somber and a\n",
            "\n",
            "Epoch 67  Batch 2176/2176:  Train Loss 4.072 Valid Loss 4.999 [Time 194.4s]\n",
            "Random sample:  the first two sessions were in their place , both as\n",
            "\n",
            "Epoch 68  Batch 2176/2176:  Train Loss 4.072 Valid Loss 4.999 [Time 194.8s]\n",
            "Random sample:  the united league from november 2006 through january 2009 , after\n",
            "\n",
            "Epoch 69  Batch 2176/2176:  Train Loss 4.070 Valid Loss 5.001 [Time 194.4s]\n",
            "Random sample:  the film and became involved by critics ' admirers for their\n",
            "\n",
            "Epoch 70  Batch 2176/2176:  Train Loss 4.067 Valid Loss 5.001 [Time 194.1s]\n",
            "Random sample:  the united nations on february 1 and was subsequently opened for\n",
            "\n",
            "Epoch 71  Batch 2176/2176:  Train Loss 4.067 Valid Loss 5.000 [Time 194.2s]\n",
            "Random sample:  the same reason , but is unsure to his left arm\n",
            "\n",
            "Epoch 72  Batch 2176/2176:  Train Loss 4.062 Valid Loss 5.001 [Time 194.3s]\n",
            "Random sample:  the game was not entirely nominated to include three players :\n",
            "\n",
            "Epoch 73  Batch 2176/2176:  Train Loss 4.062 Valid Loss 5.001 [Time 194.4s]\n",
            "Random sample:  the united territory and europe in april 2005 as a result\n",
            "\n",
            "Epoch 74  Batch 2176/2176:  Train Loss 4.058 Valid Loss 5.002 [Time 194.8s]\n",
            "Random sample:  the united sun to a tropical storm warning formed at its\n",
            "\n",
            "Epoch 75  Batch 2176/2176:  Train Loss 4.058 Valid Loss 5.003 [Time 194.4s]\n",
            "Random sample:  the same time to increase them at their distal on their\n",
            "\n",
            "Epoch 76  Batch 2176/2176:  Train Loss 4.060 Valid Loss 5.004 [Time 194.3s]\n",
            "Random sample:  the city 's transportation are used only by a local number\n",
            "\n",
            "Epoch 77  Batch 2176/2176:  Train Loss 4.058 Valid Loss 5.003 [Time 194.4s]\n",
            "Random sample:  the game is based around this time — a young man\n",
            "\n",
            "Epoch 78  Batch 2176/2176:  Train Loss 4.055 Valid Loss 5.003 [Time 194.7s]\n",
            "Random sample:  the same shortages on september 7 with its low intensity on\n",
            "\n",
            "Epoch 79  Batch 2176/2176:  Train Loss 4.054 Valid Loss 5.003 [Time 194.4s]\n",
            "Random sample:  the film and his band members also had to be filmed\n",
            "\n",
            "Epoch 80  Batch 2176/2176:  Train Loss 4.050 Valid Loss 5.003 [Time 194.7s]\n",
            "Random sample:  the same year before his death and took a place for\n",
            "\n",
            "Epoch 81  Batch 2176/2176:  Train Loss 4.050 Valid Loss 5.004 [Time 194.3s]\n",
            "Random sample:  the game and had its genesis ever met in its history\n",
            "\n",
            "Epoch 82  Batch 2176/2176:  Train Loss 4.050 Valid Loss 5.004 [Time 194.7s]\n",
            "Random sample:  the episode a mixed and critical response ; however it had\n",
            "\n",
            "Epoch 83  Batch 2176/2176:  Train Loss 4.048 Valid Loss 5.005 [Time 194.9s]\n",
            "Random sample:  the song had been released from september to september 2009 when\n",
            "\n",
            "Epoch 84  Batch 2176/2176:  Train Loss 4.046 Valid Loss 5.004 [Time 194.3s]\n",
            "Random sample:  the song dynasty was composed at $ 50 in the northern\n",
            "\n",
            "Epoch 85  Batch 2176/2176:  Train Loss 4.045 Valid Loss 5.005 [Time 194.9s]\n",
            "Random sample:  the episode 's title was mixed for three weeks : two\n",
            "\n",
            "Epoch 86  Batch 2176/2176:  Train Loss 4.043 Valid Loss 5.006 [Time 194.7s]\n",
            "Random sample:  the united league in november 2007 for england under a league\n",
            "\n",
            "Epoch 87  Batch 2176/2176:  Train Loss 4.041 Valid Loss 5.006 [Time 194.9s]\n",
            "Random sample:  the first three games of his first season , which was\n",
            "\n",
            "Epoch 88  Batch 2176/2176:  Train Loss 4.042 Valid Loss 5.005 [Time 194.8s]\n",
            "Random sample:  the second game was the last ever released of their compilation\n",
            "\n",
            "Epoch 89  Batch 2176/2176:  Train Loss 4.039 Valid Loss 5.007 [Time 194.1s]\n",
            "Random sample:  the first three seasons to have been established to a halt\n",
            "\n",
            "Epoch 90  Batch 2176/2176:  Train Loss 4.101 Valid Loss 4.989 [Time 195.0s]\n",
            "Random sample:  the same month , she participated with a large amount that\n",
            "\n",
            "Epoch 91  Batch 2176/2176:  Train Loss 4.125 Valid Loss 4.986 [Time 194.7s]\n",
            "Random sample:  the second time the player to score on its last week\n",
            "\n",
            "Epoch 92  Batch 2176/2176:  Train Loss 4.135 Valid Loss 4.980 [Time 194.9s]\n",
            "Random sample:  the film follows its cinematography release of her career to the\n",
            "\n",
            "Epoch 93  Batch 2176/2176:  Train Loss 4.134 Valid Loss 4.976 [Time 195.2s]\n",
            "Random sample:  the game was a critical or drama 's failure on which\n",
            "\n",
            "Epoch 94  Batch 2176/2176:  Train Loss 4.130 Valid Loss 4.974 [Time 194.8s]\n",
            "Random sample:  the film as an inspiration on how it could make the\n",
            "\n",
            "Epoch 95  Batch 2176/2176:  Train Loss 4.126 Valid Loss 4.973 [Time 195.1s]\n",
            "Random sample:  the song had to reach their third album and their only\n",
            "\n",
            "Epoch 96  Batch 2176/2176:  Train Loss 4.124 Valid Loss 4.972 [Time 195.2s]\n",
            "Random sample:  the end for which he scored five years after that the\n",
            "\n",
            "Epoch 97  Batch 2176/2176:  Train Loss 4.123 Valid Loss 4.972 [Time 195.0s]\n",
            "Random sample:  the second and ninth single from its fifth week and was\n",
            "\n",
            "Epoch 98  Batch 2176/2176:  Train Loss 4.120 Valid Loss 4.971 [Time 194.8s]\n",
            "Random sample:  the game and its sound were designed from scratch and the\n",
            "\n",
            "Epoch 99  Batch 2176/2176:  Train Loss 4.119 Valid Loss 4.971 [Time 195.3s]\n",
            "Random sample:  the end , the player controls out with three or five\n",
            "\n",
            "Epoch 100 Batch 2176/2176:  Train Loss 4.118 Valid Loss 4.971 [Time 195.2s]\n",
            "Random sample:  the end for which the water has not yet the first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdSc3W6ufKh4",
        "colab_type": "code",
        "outputId": "17db679e-4e09-4034-b8a9-57c81b1d25bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "example1 = 'united states is a'\n",
        "PredictLastWord(model, example1)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-3.0596, -3.2084, -3.3047, -4.0073, -4.0813, -4.2020, -4.3376, -4.4397,\n",
              "         -4.5406, -4.5797, -4.5951, -4.6088, -4.8496, -4.8573, -4.8644, -4.9803,\n",
              "         -5.0343, -5.0644, -5.0869, -5.1010], device='cuda:0',\n",
              "        grad_fn=<TopkBackward>),\n",
              " ['single',\n",
              "  'major',\n",
              "  '<unk>',\n",
              "  '\"',\n",
              "  'popular',\n",
              "  'national',\n",
              "  'series',\n",
              "  'commercial',\n",
              "  'large',\n",
              "  'state',\n",
              "  'common',\n",
              "  'part',\n",
              "  'modern',\n",
              "  'country',\n",
              "  'non',\n",
              "  'song',\n",
              "  '2',\n",
              "  'mixture',\n",
              "  'record',\n",
              "  'very'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRYwllUH9wny",
        "colab_type": "text"
      },
      "source": [
        "# Upload Saved Model and Generate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiXX5OKxWZ2n",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "b0bf2ccf-be62-4e44-c11f-1c7a63478b6c"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-803f0917-7fe9-4136-b56a-20663fe68266\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-803f0917-7fe9-4136-b56a-20663fe68266\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving best_lm_model.pth to best_lm_model.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhS0kd9pWpwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTMModel(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, \\\n",
        "                      hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS)\n",
        "model.load_state_dict(torch.load('best_lm_model.pth'))\n",
        "model = model.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVi19KfZ98MO",
        "colab_type": "text"
      },
      "source": [
        "## Predict the last word of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8JGEoGxZVl7",
        "colab_type": "code",
        "outputId": "182296f5-8828-42cc-c05a-ef4e697076d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "example2 = 'i live in new'\n",
        "PredictLastWord(model, example2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.1895, -2.9968, -3.0378, -4.0996, -4.7299, -5.2429, -5.6764, -5.8815,\n",
              "         -5.9518, -6.0597, -6.0897, -6.3670, -7.0204, -7.0247, -7.0433, -7.3135,\n",
              "         -7.4211, -7.4310, -7.4386, -7.5110], device='cuda:0',\n",
              "        grad_fn=<TopkBackward>),\n",
              " ['york',\n",
              "  'zealand',\n",
              "  'england',\n",
              "  'jersey',\n",
              "  'orleans',\n",
              "  'south',\n",
              "  'mexico',\n",
              "  'world',\n",
              "  'guinea',\n",
              "  'rochelle',\n",
              "  '<unk>',\n",
              "  'delhi',\n",
              "  'year',\n",
              "  'garden',\n",
              "  'music',\n",
              "  'britain',\n",
              "  'age',\n",
              "  'hampshire',\n",
              "  'spain',\n",
              "  'life'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yHAPOjw-Bdm",
        "colab_type": "text"
      },
      "source": [
        "## Generate random sentences starting with a seed word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkGApIi3ZkEv",
        "colab_type": "code",
        "outputId": "e2a2c57d-537a-4104-e03f-5940d7440ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "# Generate 10 random sentences starting with a seed\n",
        "seed_words = ['he', 'she']\n",
        "for word_ in seed_words:\n",
        "    print('='*90)\n",
        "    print('Seed Word: ', word_)\n",
        "    print('='*90)\n",
        "    for i in range(10):\n",
        "        print(GenerateRandom(model, word_, 15))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Seed Word:  he\n",
            "==========================================================================================\n",
            "he has since been appointed to his home of their children 's grandfather by james de\n",
            "he was able for their second attempt with their former teammates and team partner of an\n",
            "he is not considered an honorary doctorate ; his most influential work has come into three\n",
            "he returned home for two seasons in three appearances on september 2 for a record eighth\n",
            "he returned as to whether he would go up and sell to the other 's ,\n",
            "he had the first and largest most famous player award , since he won four games\n",
            "he also wrote , he is a welcome to an elderly film and was not an\n",
            "he could only live up for another session , the only match the team could play\n",
            "he also made appearances for an unnamed new team as he did it for their first\n",
            "he has since the only one of their own children of birth and family as to\n",
            "==========================================================================================\n",
            "Seed Word:  she\n",
            "==========================================================================================\n",
            "she could do something to make it personally 's not the first , and he did\n",
            "she also has two well received : she performed her career with two more songs in\n",
            "she has a high school on which is also the site at that time the world\n",
            "she began composing her as an elderly in his work , in early october 2012 ,\n",
            "she also used for this purpose for an attempt from one another to another of their\n",
            "she had been selected the album on a hot 100 chart ; he had three other\n",
            "she has written a different cover , the song is set as a duet , which\n",
            "she was able with his father from his son 's daughter 's wife at compostela park\n",
            "she also had an alternative vocal success , but he was a great admirer in the\n",
            "she could take over to join that it will do not be in an official engagement\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dQZhKg-F0T",
        "colab_type": "text"
      },
      "source": [
        "## Generate sentences using Beam Search, starting with a seed phrase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa1Yah5gTibR",
        "colab_type": "code",
        "outputId": "228ea9be-f446-4ad3-8458-fb9e97f9b16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        }
      },
      "source": [
        "seed_phrase = 'the war was won by'\n",
        "result = BeamSearch(model, seed_phrase, sentence_len=20, beam_size=100)\n",
        "for sent_ in result:\n",
        "    print(sent_)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigades ) and the 3rd light horse\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 1st and 2nd\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades ( 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigades ) , and the 3rd light\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , under the command\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades and the 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) and the 3rd light\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th light horse\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted rifles\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigade ) , and the 3rd light\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st light horse brigade , and the 1st light horse brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 11th battalion\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 1st and\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 2nd light horse\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigades ) and the 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , commanded by lieutenant\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 3rd light\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades and the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades from the 1st and 2nd\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , and the 5th\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 1st battalion\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , and the 2nd light\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , the 1st and 2nd\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigades ) , and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th mounted rifles\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades ( the 5th division and\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , under the command of\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th division and\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , under the command of\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades ( the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigade , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades and the 1st light horse\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 1st and\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , with the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , and the 1st light\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) , and the 3rd\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades and the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , and the 4th battalion\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , the 5th mounted rifles\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , and the 1st\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades ( the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades ( anzac mounted division ) and the 3rd light horse brigade , and\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) , and the 1st\n",
            "the war was won by the 1st and 2nd light horse brigades ( anzac mounted division ) and the 3rd light horse brigade , the\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades and the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) , and the 5th\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigade , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , the 5th mounted\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , the 5th mounted division\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st light horse brigade , the 5th mounted brigade , and\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades ) , the 1st battalion\n",
            "the war was won by the 1st and 2nd light horse brigades ( anzac mounted division ) and the 3rd light horse brigade , which\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , under the command of\n",
            "the war was won by the 1st and 2nd light horse brigades ( anzac mounted division ) and the 3rd light horse brigade , the\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 11th battalion ,\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , and the 1st battalion\n",
            "the war was won by the 1st and 2nd light horse brigade , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigade , the 5th division and\n",
            "the war was won by the 1st and 2nd light horse brigades ( 1st and 2nd light horse brigades ) , and the 1st battalion\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 1st and 2nd light horse brigades , and the 1st\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , which was supported by\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , under the command of\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th division ,\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 2nd battalion ,\n",
            "the war was won by the 1st and 2nd light horse brigades , the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades and the 1st and 2nd light horse brigades , the 5th mounted brigade\n",
            "the war was won by the 1st and 2nd light horse brigades , and the 5th mounted brigade and the 1st light horse brigade ,\n",
            "the war was won by the 1st and 2nd light horse brigades ( the 1st and 2nd light horse brigades , the 5th division ,\n",
            "the war was won by the 1st and 2nd light horse brigades ( anzac mounted division ) and the 3rd light horse brigade and the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3olZsou2fd1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('best_lm_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyU1TDIXiz-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}